\documentclass[11pt]{scrartcl} % Font size
\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{
	\normalfont\normalsize
	\textsc{Harvard Privacy Tools Project}\\ % Your university, school and/or department name(s)
	\vspace{25pt} % Whitespace
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{20pt} % Whitespace
	{\huge Noise Generation Notes}\\ % The assignment title
	\vspace{12pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{12pt} % Whitespace
}

\author{\LARGE Christian Covington} % Your name

\date{\normalsize\today} % Today's date (\today) or a custom date

\begin{document}

\maketitle

\section{Overview}
This document is a write-up of extra notes regarding the ways in which we sample noise in yarrow.

\section{Random Number Generation}
All of our random number generation involves uniform random sampling of bits via OpenSSL.
We will take as given that OpenSSL is cryptographically secure, and talk about how it
forms the basis for various functions in the library.
When we refer to floating-point numbers, we specifically mean the IEEE 754 floating-point standard.

\subsection{Uniform Number Generation}

\subsubsection{$sample\_uniform(min: f64, max: f64)$}
This is our basic uniform random number generator. We get 8 bytes (64 bits) from OpenSSL and interpret
this as as an unsigned 64-bit integer $m$. We divide $m$ by $2^{63}-1$ to get $m' \in [0,1]$, and then
rescale with both a multiplicative and additive factor to generate a value within the desired range,
that is $m'' = m' \cdot (max - min) + min$, where min and max are user-provided data bounds.
We then return $m''$. \newline

Note that not every floating-point number between min and max is able to
be generated by this method. We will always get something of the form
\[ m'' = \frac{m}{2^{63}-1} * (max - min) + min, \]
where $m \in \{0, 1, \hdots, 2^{63}-1\}$. Each possible $m''$ occurs with probability $\frac{1}{2^{64}}$.

\subsubsection{$sample\_uniform\_snapping()$}
This is the uniform random number generator described in \cite{Mir12}.
Unlike $sample\_uniform$, which takes in arbitrary min and max, this function
always generates numbers in $[0,1)$.\footnote{Of course, it could be scaled to arbitrary ranges through the same
multiplicative and additive scaling as $sample\_uniform$.} \newline

In this method, all normal floating-point numbers are representable, though they are not uniformly generated
over the set of possible values.
Instead, each is generated with probability relative to its unit of least precision.\footnote{The ULP is the value
represented by the least significant bit of the mantissa if that bit is a 1.}
That is, we generate numbers in $[\frac{1}{2},1)$ with probability $\frac{1}{2}$, numbers in $[\frac{1}{4}, \frac{1}{2})$
with probability $\frac{1}{4}$, etc. Within each precision band (the set of numbers with the same unit
of least precision), numbers are sampled uniformly.
We achieve this sample our exponent from a geometric distribution with parameter $p = 0.5$ and a mantissa uniformly from $\{0,1\}^{52}$.
Let $e$ be a draw from $Geom(0.5)$ (truncated such that $e \leq 1023$) and $m_1, m_2 \hdots, m_{52}$ be the bits of our mantissa. Then our function outputs $u$, where
\[ u = (1.m_1m_2 \hdots m_{52})_2 * 2^{-e}. \]

\subsection{Biased Bit Sampling}
Recall that we are taking as given that we are able to sample uniform bits from OpenSSL.
For many applications, however, we want to be able to sample bits non-uniformly,
i.e. where $\Pr(bit = 1) \neq \frac{1}{2}$. To do so, we use the $sample\_bit$ function.

\subsubsection{$sample\_bit(prob: f64)$}
This function uses the unbiased bit generation from OpenSSL to return a single bit, where $\Pr(bit = 1) = prob$.
I was introduced to the method for biasing an unbiased coin from a homework assignment given by Michael Mitzenmacher,
and I later found a write-up online \href{https://amakelov.wordpress.com/2013/10/10/arbitrarily-biasing-a-coin-in-2-expected-tosses/}{here}.
We will give a general form of the algorithm, and then talk about implementation details.
\begin{algorithm}[H]
	\caption{Biasing an unbiased coin}
	\label{alg:biasing_a_coin}
	\begin{algorithmic}[1]
		\State $p \gets \Pr(bit = 1)$
		\State Find the infinite binary expansion of $p$, which we call $b = (b_1, b_2, \hdots,)_2$.
		Note that $p = \sum_{i=1}^{\infty}\frac{b_i}{2^i}$.
		\State Toss an unbiased coin until the first instance of ``heads''. Call the (1-based) index where this occurred $k$.
		\If {$b_k = 1$}
			\State return $1$
		\Else
			\State return $0$
		\EndIf
	\end{algorithmic}
\end{algorithm}
Let's first show that this procedure gives the correct expectation:
\begin{align*}
	prob &= \Pr(bit = 1) \\
		 &= \sum_{i=1}^{\infty} \Pr(bit = 1 \vert k = i) \Pr(k = i) \\
		 &= \sum_{i=1}^{\infty} b_i \cdot \frac{1}{2^i} \\
		 &= \sum_{i=1}^{\infty}\frac{b_i}{2^i}.
\end{align*}
This is consistent with the statement in Algorithm~\ref{alg:biasing_a_coin}, so we know that
the process returns bits with the correct bias.
In terms of efficiency, we know that we can stop coin flipping once we get a heads,
so that part of the algorithm hsa $\E(\# flips) = 2$. \newline

The part that is a bit more difficult is constructing the infinite binary expansion of $p$.
We start by noting that, for our purposes, we do not actually need an infinite binary expansion.
Because $p$ will always be a 64-bit floating-point number, we need only get a binary
expansion that covers all representable numbers in our floating-point standard that are
also valid probabilities.
Luckily, the underlying structure of floating-point numbers makes this quite easy. \newline

In the 64-bit standard, floating-point numbers in $[0,1]$ are represented as
\[ (1.m_1m_2 \hdots m_{52})_2 * 2^{(e_{1}e_2 \hdots e_{11})_2 - 1023}. \]
Then, our binary expansion is just our mantissa $(1.m_1m_2 \hdots m_{52})_2$, with
the radix point shifted based on the value of the exponent.
We can then index into the properly shifted mantissa and check the value of the $k$th element.


\section{Truncation vs. Censoring}
Throughout our noise functions, we use the terms \emph{truncated} and \emph{censored}.
Both are means of bounding the support of the noise distribution, but they are distinct. \newline

Truncating a distribution simply ignores events outside of the given bounds, so
all probabilities within the given bounds are scaled up by a constant factor.
One way to generate a truncated distribution is via rejection sampling.
You can generate samples from a probability distribution as you normally would (without any bounding),
and reject any sample that falls outside of your bounds. \newline

Censoring a distribution, rather than ignoring events outside of the given bounds, pushes the
probabilities of said events to the closest event within the given bounds. One way to generate
a censored distribution would be to generate samples from a probability distribution as you
typically would, and then clamp samples that fall outside of your bounds to the closest element
inside your bounds.



\bibliographystyle{alpha}
\bibliography{noise}
\end{document}